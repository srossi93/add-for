{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 345,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-10-14T00:57:18.025906Z",
     "start_time": "2017-10-13T20:57:18.017793-04:00"
    },
    "collapsed": true,
    "run_control": {
     "frozen": false,
     "read_only": false
    }
   },
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "\n",
    "from tensorflow.contrib import layers\n",
    "from tensorflow.contrib.framework.python.ops import arg_scope\n",
    "from tensorflow.contrib.layers.python.layers import layers as layers_lib\n",
    "\n",
    "from keras.datasets import cifar10, mnist"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "run_control": {
     "frozen": false,
     "read_only": false
    }
   },
   "source": [
    "# Write TFRecord"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 286,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-10-12T03:15:59.853395Z",
     "start_time": "2017-10-11T23:15:59.815599-04:00"
    },
    "collapsed": true,
    "run_control": {
     "frozen": false,
     "read_only": false
    }
   },
   "outputs": [],
   "source": [
    "# Modified code from TensorFlow Repo\n",
    "# Reference: tensorflow/examples/how_tos/reading_data/convert_to_records.py\n",
    "def _int64_feature(value):\n",
    "    return tf.train.Feature(float_list=tf.train.FloatList(value=[value]))\n",
    "\n",
    "def _bytes_feature(value):\n",
    "    return tf.train.Feature(bytes_list=tf.train.BytesList(value=[value]))\n",
    "\n",
    "def convert_to(data_set, name):\n",
    "    \"\"\"Converts a dataset to tfrecords.\"\"\"\n",
    "    images = data_set[0]\n",
    "    labels = data_set[1]\n",
    "    num_examples = data_set[2]\n",
    "\n",
    "    if images.shape[0] != num_examples:\n",
    "        raise ValueError('Images size %d does not match label size %d.' %\n",
    "                     (images.shape[0], num_examples))\n",
    "    rows = images.shape[1]\n",
    "    cols = images.shape[2]\n",
    "    if len(images.shape) == 4:\n",
    "        depth = images.shape[3]\n",
    "    else:\n",
    "        depth = 1\n",
    "\n",
    "    filename =  name + '.tfrecords'\n",
    "    print('Writing', filename, end=' ')\n",
    "    writer = tf.python_io.TFRecordWriter(filename)\n",
    "    for index in range(num_examples):\n",
    "        image_raw = images[index].tostring()\n",
    "        example = tf.train.Example(features=tf.train.Features(feature={\n",
    "            'height': _int64_feature(rows),\n",
    "            'width': _int64_feature(cols),\n",
    "            'depth': _int64_feature(depth),\n",
    "            'label': _int64_feature(int(labels[index])),\n",
    "            'image_raw': _bytes_feature(image_raw)}))\n",
    "        writer.write(example.SerializeToString())\n",
    "    writer.close()\n",
    "    print('done.')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 287,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-10-12T03:16:00.533933Z",
     "start_time": "2017-10-11T23:16:00.273284-04:00"
    },
    "collapsed": true,
    "run_control": {
     "frozen": false,
     "read_only": false
    }
   },
   "outputs": [],
   "source": [
    "(x_train, y_train), (x_test, y_test) = cifar10.load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 359,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-10-14T01:26:45.142572Z",
     "start_time": "2017-10-13T21:26:45.135479-04:00"
    },
    "run_control": {
     "frozen": false,
     "read_only": false
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10000, 1)"
      ]
     },
     "execution_count": 359,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 384,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-10-14T01:45:40.851162Z",
     "start_time": "2017-10-13T21:44:42.672978-04:00"
    },
    "run_control": {
     "frozen": false,
     "read_only": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing train.tfrecords done.\n",
      "Writing test.tfrecords done.\n"
     ]
    }
   ],
   "source": [
    "convert_to([x_train, y_train, len(x_train)], 'train')\n",
    "convert_to([x_test, y_test, len(x_test)], 'test')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "run_control": {
     "frozen": false,
     "read_only": false
    }
   },
   "source": [
    "# Read TFRecord"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 332,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-10-12T03:35:15.141734Z",
     "start_time": "2017-10-11T23:35:15.110011-04:00"
    },
    "collapsed": true,
    "run_control": {
     "frozen": false,
     "read_only": false
    }
   },
   "outputs": [],
   "source": [
    "# Modified code from TensorFlow Repo\n",
    "# Reference: tensorflow/examples/how_tos/reading_data/fully_connected_reader.py\n",
    "\n",
    "def read_and_decode(filename_queue, shape):\n",
    "    reader = tf.TFRecordReader()\n",
    "    _, serialized_example = reader.read(filename_queue)\n",
    "    features = tf.parse_single_example(serialized_example,\n",
    "                                       features={'image_raw': tf.FixedLenFeature([], tf.string),\n",
    "                                                 'label': tf.FixedLenFeature([], tf.float32),})\n",
    "\n",
    "    # Convert from a scalar string tensor (whose single string has\n",
    "    # length mnist.IMAGE_PIXELS) to a uint8 tensor with shape\n",
    "    # [mnist.IMAGE_PIXELS].\n",
    "    image = tf.decode_raw(features['image_raw'], tf.uint8)\n",
    "    image = tf.reshape(image, shape)\n",
    "    #image.set_shape([image_pixels])\n",
    "\n",
    "    # Convert from [0, 255] -> [-0.5, 0.5] floats.\n",
    "    image = tf.cast(image, tf.float32) * (1. / 255) - 0.5\n",
    "\n",
    "    # Convert label from a scalar uint8 tensor to an int32 scalar.\n",
    "    label = tf.cast(features['label'], tf.int32)\n",
    "    label = tf.one_hot(label, 10)\n",
    "    #label = tf.reshape(image, [1])\n",
    "    return image, label\n",
    "\n",
    "\n",
    "def inputs(train, batch_size, num_epochs, filename, shape):\n",
    "    if not num_epochs: num_epochs = None\n",
    "\n",
    "    with tf.name_scope('input'):\n",
    "        filename_queue = tf.train.string_input_producer(\n",
    "        [filename], num_epochs=num_epochs)\n",
    "\n",
    "    # Even when reading in multiple threads, share the filename\n",
    "    # queue.\n",
    "    image, label = read_and_decode(filename_queue, shape)\n",
    "\n",
    "    # Shuffle the examples and collect them into batch_size batches.\n",
    "    # (Internally uses a RandomShuffleQueue.)\n",
    "    # We run this in two threads to avoid being a bottleneck.\n",
    "    images, sparse_labels = tf.train.shuffle_batch(\n",
    "        [image, label], batch_size=batch_size, num_threads=2,\n",
    "        capacity=1000 + 3 * batch_size,\n",
    "        # Ensures a minimum amount of shuffling of examples.\n",
    "        min_after_dequeue=1000)\n",
    "    #sparse_labels = tf.reshape(sparse_labels, [-1, 10])\n",
    "    return images, sparse_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 333,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-10-12T03:35:16.033418Z",
     "start_time": "2017-10-11T23:35:15.945836-04:00"
    },
    "collapsed": true,
    "run_control": {
     "frozen": false,
     "read_only": false
    }
   },
   "outputs": [],
   "source": [
    "images, labels = inputs(train=True, batch_size=128,  num_epochs=5, \n",
    "                        filename='./train.tfrecords', shape=[32, 32, 3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 292,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-10-12T03:17:10.851276Z",
     "start_time": "2017-10-11T23:17:10.846467-04:00"
    },
    "run_control": {
     "frozen": false,
     "read_only": false
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor 'shuffle_batch_1:0' shape=(128, 32, 32, 3) dtype=float32>"
      ]
     },
     "execution_count": 292,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 293,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-10-12T03:17:12.692687Z",
     "start_time": "2017-10-11T23:17:12.687237-04:00"
    },
    "run_control": {
     "frozen": false,
     "read_only": false
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor 'shuffle_batch_1:1' shape=(128, 10) dtype=float32>"
      ]
     },
     "execution_count": 293,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "run_control": {
     "frozen": false,
     "read_only": false
    }
   },
   "source": [
    "# Build the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 346,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-10-14T00:57:33.786790Z",
     "start_time": "2017-10-13T20:57:33.739087-04:00"
    },
    "collapsed": true,
    "run_control": {
     "frozen": false,
     "read_only": false
    }
   },
   "outputs": [],
   "source": [
    "def alex_net(inputs, n_classes, prob=1., verbose=False):\n",
    "    input_layer = tf.reshape(inputs, [-1, 32, 32, 3])\n",
    "    print(input_layer) if verbose else 0\n",
    "    net = layers.conv2d(input_layer, 64, [11, 11], 3, padding='SAME', scope='conv1', activation_fn=tf.nn.relu, \\\n",
    "                        weights_initializer=tf.truncated_normal_initializer(0, 0.1), \\\n",
    "                        biases_initializer=tf.truncated_normal_initializer(0.3, 0.1))\n",
    "    print(net) if verbose else 0\n",
    "    net = layers_lib.max_pool2d(net, [3, 3], 2, scope='pool1')\n",
    "    print(net) if verbose else 0\n",
    "    net = layers.conv2d(net, 192, [3, 3], scope='conv2', activation_fn=tf.nn.relu,  \\\n",
    "                        weights_initializer=tf.truncated_normal_initializer(0, 0.1), \\\n",
    "                        biases_initializer=tf.truncated_normal_initializer(0.3, 0.1))\n",
    "    print(net) if verbose else 0\n",
    "    net = layers_lib.max_pool2d(net, [3, 3], 1, scope='pool2')\n",
    "    print(net) if verbose else 0\n",
    "    net = layers.conv2d(net, 384, [3, 3], scope='conv3', activation_fn=tf.nn.relu,  \\\n",
    "                        weights_initializer=tf.truncated_normal_initializer(0, 0.1), \\\n",
    "                        biases_initializer=tf.truncated_normal_initializer(0.3, 0.1))\n",
    "    print(net) if verbose else 0\n",
    "    net = layers.conv2d(net, 384, [3, 3], scope='conv4', activation_fn=tf.nn.relu,  \\\n",
    "                        weights_initializer=tf.truncated_normal_initializer(0, 0.1), \\\n",
    "                        biases_initializer=tf.truncated_normal_initializer(0.3, 0.1))\n",
    "    print(net) if verbose else 0\n",
    "    net = layers.conv2d(net, 256, [3, 3], scope='conv5', activation_fn=tf.nn.relu,  \\\n",
    "                        weights_initializer=tf.truncated_normal_initializer(0, 0.1), \\\n",
    "                        biases_initializer=tf.truncated_normal_initializer(0.3, 0.1))\n",
    "    print(net) if verbose else 0\n",
    "    net = layers_lib.max_pool2d(net, [3, 3], 2, scope='pool5')\n",
    "    print(net) if verbose else 0\n",
    "    net = layers.fully_connected(net, num_outputs=4096, scope='fc1', activation_fn=tf.nn.relu,  \\\n",
    "                        weights_initializer=tf.truncated_normal_initializer(0, 0.1), \\\n",
    "                        biases_initializer=tf.truncated_normal_initializer(0.3, 0.1))\n",
    "    print(net) if verbose else 0\n",
    "    net = layers.dropout(net, prob)\n",
    "    print(net) if verbose else 0\n",
    "    net = layers.fully_connected(net, num_outputs=4096, scope='fc2', activation_fn=tf.nn.relu,  \\\n",
    "                        weights_initializer=tf.truncated_normal_initializer(0, 0.1), \\\n",
    "                        biases_initializer=tf.truncated_normal_initializer(0.3, 0.1))\n",
    "    print(net) if verbose else 0\n",
    "    net = layers.dropout(net, prob)\n",
    "    print(net) if verbose else 0\n",
    "    net = layers.fully_connected(net, num_outputs=n_classes, scope='fc3', activation_fn=tf.nn.relu,  \\\n",
    "                        weights_initializer=tf.truncated_normal_initializer(0, 0.1), \\\n",
    "                        biases_initializer=tf.truncated_normal_initializer(0.3, 0.1))\n",
    "    print(net) if verbose else 0\n",
    "    net = layers.softmax(net)\n",
    "    print(net) if verbose else 0\n",
    "    return tf.reshape(net, [-1, 10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 400,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-10-14T02:00:46.046120Z",
     "start_time": "2017-10-13T22:00:45.962834-04:00"
    },
    "collapsed": true,
    "run_control": {
     "frozen": false,
     "read_only": false
    }
   },
   "outputs": [],
   "source": [
    "def train(learning_rate=0.01, training_epochs=1, batch_size=128, \\\n",
    "          display_step=1, logs_path='./logs', keep_probability= 0.5, num_samples=50000, training_time=120):\n",
    "\n",
    "    tf.reset_default_graph()\n",
    "    \n",
    "    x, y = inputs(train=True, batch_size=batch_size, num_epochs=training_epochs*2, \\\n",
    "                                filename='./train.tfrecords', shape=[32, 32, 3])\n",
    "    x_test, y_test = inputs(train=False, batch_size=batch_size, num_epochs=training_epochs*2, \\\n",
    "                                filename='./test.tfrecords', shape=[32, 32, 3])\n",
    "\n",
    "    pred = alex_net(x, 10, keep_probability)\n",
    "    pred_test = alex_net(x_test, 10, keep_probability)\n",
    "    \n",
    "    #Cross entropy loss\n",
    "    loss = tf.reduce_mean(-tf.reduce_sum(y*tf.log(pred+1e-9), reduction_indices=1))\n",
    "\n",
    "    optimizer = tf.train.GradientDescentOptimizer(learning_rate).minimize(loss)\n",
    "    accuracy_train = tf.reduce_mean(tf.cast(tf.equal(tf.argmax(pred, 1), tf.argmax(y, 1)), tf.float32))\n",
    "    accuracy_test = tf.reduce_mean(tf.cast(tf.equal(tf.argmax(pred_test, 1), tf.argmax(y_test, 1)), tf.float32))\n",
    "    \n",
    "    init = tf.group(tf.global_variables_initializer(), tf.local_variables_initializer())\n",
    "\n",
    "    tf.summary.scalar(\"Loss\", loss)\n",
    "    tf.summary.scalar(\"Accuracy_Train\", accuracy_train)\n",
    "    tf.summary.scalar(\"Accuracy_Test\", accuracy_test)\n",
    "    \n",
    "    merged_summary_op = tf.summary.merge_all()\n",
    "    \n",
    "    saver = tf.train.Saver()\n",
    "    total_parameters = 0\n",
    "    for variable in tf.trainable_variables():\n",
    "        shape = variable.get_shape()\n",
    "        variable_parameters = 1\n",
    "        for dim in shape:\n",
    "            variable_parameters *= dim.value\n",
    "        total_parameters += variable_parameters\n",
    "    print('Num of parameters:', total_parameters)\n",
    "    \n",
    "    with tf.Session() as session:\n",
    "        summary_writer = tf.summary.FileWriter(logs_path, graph=tf.get_default_graph())\n",
    "        session.run(init)\n",
    "        coord = tf.train.Coordinator()\n",
    "        threads = tf.train.start_queue_runners(sess=session, coord=coord)\n",
    "        \n",
    "        step = 0\n",
    "        start_time = time.time()\n",
    "\n",
    "        total_iterations = int(num_samples / batch_size * training_epochs)\n",
    "        print('Num of steps:', total_iterations)\n",
    "        for step in range(total_iterations):\n",
    "            _, loss_value, summary = session.run([optimizer, loss, merged_summary_op])\n",
    "            duration = time.time() - start_time         \n",
    "            if step % display_step == 0:\n",
    "                print('Step %d: loss = %.2f - accuracy train = %.2f - accuracy test =  %.2f (%.3f sec)' % \n",
    "                          (step, loss_value, accuracy_train.eval(), accuracy_test.eval(), duration))\n",
    "            if duration/60 > training_time: \n",
    "                print('Training time exceeded. Stopping now...')\n",
    "                break\n",
    "            summary_writer.add_summary(summary, step)\n",
    "            saver.save(sess, './models/model_' + str(learning_rate) + '_' + str(batch_size))\n",
    "        coord.request_stop()\n",
    "        coord.join(threads)\n",
    "        session.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 401,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-10-14T02:01:54.208414Z",
     "start_time": "2017-10-13T22:00:46.376835-04:00"
    },
    "run_control": {
     "frozen": false,
     "read_only": false
    },
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num of parameters: 20885450\n",
      "Num of steps: 39062\n",
      "Step 0: loss = 19.27 - accuracy train = 0.10 - accuracy test =  0.13 (2.176 sec)\n",
      "Step 1: loss = 18.29 - accuracy train = 0.13 - accuracy test =  0.11 (4.541 sec)\n",
      "Step 2: loss = 17.81 - accuracy train = 0.11 - accuracy test =  0.07 (7.127 sec)\n",
      "Step 3: loss = 19.27 - accuracy train = 0.11 - accuracy test =  0.12 (9.671 sec)\n",
      "Step 4: loss = 18.29 - accuracy train = 0.11 - accuracy test =  0.07 (12.183 sec)\n",
      "Step 5: loss = 18.94 - accuracy train = 0.13 - accuracy test =  0.09 (14.775 sec)\n",
      "Step 6: loss = 18.46 - accuracy train = 0.12 - accuracy test =  0.08 (17.714 sec)\n",
      "Step 7: loss = 18.78 - accuracy train = 0.14 - accuracy test =  0.02 (21.164 sec)\n",
      "Step 8: loss = 18.22 - accuracy train = 0.05 - accuracy test =  0.10 (24.610 sec)\n",
      "Step 9: loss = 19.27 - accuracy train = 0.09 - accuracy test =  0.09 (27.394 sec)\n",
      "Step 10: loss = 19.27 - accuracy train = 0.15 - accuracy test =  0.10 (30.096 sec)\n",
      "Step 11: loss = 18.62 - accuracy train = 0.12 - accuracy test =  0.16 (32.826 sec)\n",
      "Step 12: loss = 19.10 - accuracy train = 0.11 - accuracy test =  0.12 (35.548 sec)\n",
      "Step 13: loss = 18.78 - accuracy train = 0.09 - accuracy test =  0.12 (38.284 sec)\n",
      "Step 14: loss = 18.62 - accuracy train = 0.10 - accuracy test =  0.06 (41.014 sec)\n",
      "Step 15: loss = 18.62 - accuracy train = 0.12 - accuracy test =  0.09 (43.649 sec)\n",
      "Step 16: loss = 18.62 - accuracy train = 0.12 - accuracy test =  0.12 (46.229 sec)\n",
      "Step 17: loss = 18.78 - accuracy train = 0.12 - accuracy test =  0.05 (48.837 sec)\n",
      "Step 18: loss = 19.27 - accuracy train = 0.07 - accuracy test =  0.15 (51.463 sec)\n",
      "Step 19: loss = 19.10 - accuracy train = 0.09 - accuracy test =  0.07 (54.306 sec)\n",
      "Step 20: loss = 18.78 - accuracy train = 0.12 - accuracy test =  0.13 (57.027 sec)\n",
      "Step 21: loss = 19.10 - accuracy train = 0.08 - accuracy test =  0.07 (59.825 sec)\n",
      "Step 22: loss = 19.27 - accuracy train = 0.09 - accuracy test =  0.10 (64.422 sec)\n",
      "Training time exceeded. Stopping now...\n"
     ]
    }
   ],
   "source": [
    "train(batch_size=128, display_step=1, training_epochs=100, num_samples=50000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "run_control": {
     "frozen": false,
     "read_only": false
    }
   },
   "source": [
    "![](./images/loss.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "run_control": {
     "frozen": false,
     "read_only": false
    }
   },
   "source": [
    "![](./images/train.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "run_control": {
     "frozen": false,
     "read_only": false
    }
   },
   "source": [
    "![](./images/test.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  },
  "toc": {
   "colors": {
    "hover_highlight": "#DAA520",
    "running_highlight": "#FF0000",
    "selected_highlight": "#FFD700"
   },
   "moveMenuLeft": true,
   "nav_menu": {
    "height": "4px",
    "width": "254px"
   },
   "navigate_menu": true,
   "number_sections": true,
   "sideBar": true,
   "threshold": 4,
   "toc_cell": false,
   "toc_section_display": "block",
   "toc_window_display": false,
   "widenNotebook": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
